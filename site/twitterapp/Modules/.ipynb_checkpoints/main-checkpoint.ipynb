{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "install plotly and try to make some of the plots we have into those. Then try to save them as images for the report as well, I want to test the interactivity of it also.\n",
    "\n",
    "[] pip install plotly.\n",
    "\n",
    "\n",
    "reasons for using tf-idf https://stats.stackexchange.com/questions/153069/bag-of-words-for-text-classification-why-not-just-use-word-frequencies-instead good papers to read for reference.\n",
    "\n",
    "https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words another good one to read.\n",
    "\n",
    "[] download extentions for my jupyter. expecially the time one for each block will be useful to know what things i could be pickling.\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/44012099/creating-a-dataframe-from-a-dict-where-keys-are-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T23:42:02.816296Z",
     "start_time": "2017-12-21T23:42:00.612587Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tokenizer import *\n",
    "from gensim.models import Word2Vec\n",
    "%matplotlib inline\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in hashtag files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T23:42:33.741015Z",
     "start_time": "2017-12-21T23:42:33.722309Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "Finished collecting messages\n"
     ]
    }
   ],
   "source": [
    "# class parser\n",
    "# A document = twitter message\n",
    "import os\n",
    "from extract_messages import load_data_hashtags\n",
    "\n",
    "topics = ['#sports', '#politics', '#technology', '#food', '#music'] # hashtags that i want\n",
    "frames = []\n",
    "\n",
    "# check files already exist before creating one\n",
    "# use generated results.\n",
    "\n",
    "data = load_data_hashtags(topics, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Messages with tokenize & use TF-IDF on all the twitter messages\n",
    "\n",
    "Report:\n",
    "A TF-IDF score is a value in the range of 0 to 1. it is defined as [Maths equations]\n",
    "A TF-IDF is a matrix of document-word matrix where a value is given to each word 0 no importance in the document to 1 being very important to the document. A crucial method of getting the most significant words in a document is first to filter out the words with the lowest low scores in one of the rows (a document) of the TF-IDF matrix. To do this a simple threshold is applied. This is because common words might have a low tf-idf score within each document. But appearances are frequent when averaged over all documents they would otherwise dominate all other terms. [reference?]\n",
    "\n",
    "Reference the code: https://buhrmann.github.io/tfidf-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "* what is put into the TF-IDF Corpus (5 tags) is put into TFidf vectoriser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold TF-IDF score for obtaining the significant words, and get a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize\n",
    "\n",
    "data['message'] = data['message'].replace('^W\\t', '', regex=True)\n",
    "\n",
    "corpus = data['message']\n",
    "y = data['label'].values\n",
    "\n",
    "# TF-IDF documents:\n",
    "tfidf = TfidfVectorizer(decode_error='replace', strip_accents='unicode',\n",
    "                        stop_words='english', tokenizer=tokenize,\n",
    "                        max_features=1000)\n",
    "X = tfidf.fit_transform(corpus.values)\n",
    "\n",
    "# TF-IDF Features:\n",
    "features_input = np.array(tfidf.get_feature_names())\n",
    "assert len(features_input) == X.shape[1], \"number of attributes did not match with matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in ground truth topic as a bag of words and feature vector\n",
    "\n",
    "Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data/ground_feature.pkl\n",
      "loaded data/topic_bow.pkl\n"
     ]
    }
   ],
   "source": [
    "from input_output import save_topics_as_bagofwords, load_topics_as_bagofwords\n",
    "# uncomment to generate ground_feature and ground_bow, comment once it has finished\n",
    "# max feature parameter is load_topics_as_bagofwords2 in extract messages\n",
    "# save_topics_as_bagofwords(topics)\n",
    "\n",
    "ground_features, ground_bow = load_topics_as_bagofwords()\n",
    "assert np.unique(ground_features).shape[0] == ground_features.shape[0], 'feature set is not unique'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build bag of words for test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bagofwords import build_bow\n",
    "\n",
    "# bow for test data\n",
    "bow_test = build_bow(corpus, ground_features)\n",
    "\n",
    "# print np.where(bow_matrix[0] == 1)\n",
    "# print bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run main.py to collect *.csv for similiarity scores by comparing bow with given distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_output import collect_similarity_scores2\n",
    "collect_similarity_scores2(ground_bow , bow_test, confirm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### work out the best for each similarity score\n",
    "\n",
    "**implementation of model** (best converted into a diagram DONE)\n",
    "* compares the message input with ground truth messages\n",
    "* every msg input is compared (**any** similarity measure) with topic messages\n",
    "* comparisons (similarity score) for a message is summed for each topic\n",
    "* the summed sim_score for each topic are compared, the maximum is choosen.\n",
    "* msg_input -> topic.\n",
    "\n",
    "model(msg_input) -> {msg_input: topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected 9 arguments, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-400-7c51cd04078a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0memp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTwitterSim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m_make\u001b[0;34m(cls, iterable, new, len)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected 9 arguments, got 4"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import csv\n",
    "# def msg_scores\n",
    "\n",
    "#list the csv files that exist\n",
    "path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# (_, _, filenames) = os.walk(path).next()\n",
    "# csv_files = []\n",
    "# for i in filenames:\n",
    "#     if '.csv' in i:\n",
    "#         csv_files.append(i)\n",
    "\n",
    "\n",
    "TwitterSim = namedtuple('TwitterSim', ['ground_tweet_idx', 'test_tweet_idx',\n",
    "                                       'matching', 'hamming','yule', 'jaccard', \n",
    "                                       'dice', 'kulsinski', 'sokal_sneath'])\n",
    "\n",
    "csv_files = ['#food_dist.csv']\n",
    "        \n",
    "msg_total_score = {}\n",
    "\n",
    "for filename in csv_files:\n",
    "\n",
    "    # reset\n",
    "    weight = defaultdict(int) # number of times an idx appears\n",
    "    total_msg_sim_score = defaultdict(float)\n",
    "\n",
    "    csvname = filename\n",
    "    filename = os.path.join(path, filename)\n",
    "    \n",
    "    for emp in map(TwitterSim._make, csv.reader(open(filename, \"rb\"))):\n",
    "        print emp\n",
    "        break\n",
    "\n",
    "#     with open(filename, 'rb') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         reader.next()\n",
    "#         for row in reader:\n",
    "#             msg_input_idx, _, sim_score = row ###\n",
    "#             total_msg_sim_score[int(msg_input_idx)] += float(sim_score)\n",
    "#             weight[int(msg_input_idx)] += 1\n",
    "\n",
    "#     normalise_msg_score(total_msg_sim_score, weight)\n",
    "#     msg_total_score[csvname] = total_msg_sim_score\n",
    "# return msg_total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect message scores\n",
    "from membership import msg_scores\n",
    "msg_scores = msg_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from membership import find_msg_members\n",
    "msg_member = find_msg_members()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "**implementation** (best converted into a formula)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'kulsinski': 0.08548707753479125, u'yule': 0.12524850894632206, u'dice': 0.1172962226640159, u'jaccard': 0.11133200795228629, u'sokal_sneath': 0.10337972166998012, u'hamming': 0.147117296222664, u'matching': 0.147117296222664})\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from membership import performance\n",
    "performance(msg_member , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['msg_input', 'topic_msg', 'similarity score']\n",
    "\n",
    "df[df['similarity score']>=0.2]\n",
    "\n",
    "#issue which i will write a test for it, Expectation: comparison of two identical sentences is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach: clustering (not going to explore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "from kmodes.kmodes import KModes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph_file_path = os.path.join(os.getcwd(), 'graph/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**issue which i will write a test for it, Expectation: comparison of two identical sentences is one. need to fix the similarity measure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Look for the most signigficant words in a document\n",
    "\n",
    "change the TF-IDF score threshhold by setting min_tfidf\n",
    "```\n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0.4), min_tfidf=0.4) \n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0.3), min_tfidf=0.3) \n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0), min_tfidf=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tfidf_threshold import plot_tfidf_classfeats_h, top_feats_by_class\n",
    "\n",
    "# change the TF-IDF score threshhold by setting min_tfidf:\n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0.6), min_tfidf=0.6) \n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0.3), min_tfidf=0.3) \n",
    "plot_tfidf_classfeats_h(top_feats_by_class(X,y, features, min_tfidf=0), min_tfidf=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words filtered are stored\n",
    "\n",
    "and...\n",
    "\n",
    "currently we have a vector for each class. (that's not good because we want to plot more than 5 points).\n",
    "Then how?\n",
    "\n",
    "\n",
    "from the words seen above, I want to go back to the tf-idf matrix and only keep the these words across all the documents. assigning X to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_feats_by_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-821b588617eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# store top features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthresholded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_feats_by_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# include label in dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholded_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_feats_by_class' is not defined"
     ]
    }
   ],
   "source": [
    "# store top features\n",
    "thresholded_words = top_feats_by_class(X, y, features, min_tfidf=0.3)\n",
    "\n",
    "# include label in dataframe\n",
    "for df in thresholded_words:\n",
    "    df['label'] = df.label\n",
    "\n",
    "\n",
    "thresholded_words = pd.concat(thresholded_words, ignore_index=True)\n",
    "# thresholded words and labels in a numpy array\n",
    "X_ = thresholded_words['feature'].values\n",
    "y_ = thresholded_words['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtain a matrix with tweet and word\n",
    "df_features = pd.Series(features)\n",
    "\n",
    "# keep thresholded words in tfidf matrix\n",
    "# some documents are irrelevant now because we have removed terms.\n",
    "\n",
    "X_threshold_binary = convert_tfidf_to_bagofwords(X, thresholded_words_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# map feature index: word\n",
    "feature_idx_to_word = dict()\n",
    "\n",
    "features_info = thresholded_words[['feature', 'feature index']].values\n",
    "\n",
    "for word, idx in features_info:\n",
    "    feature_idx_to_word[idx] = word\n",
    "\n",
    "# find the term in which documents\n",
    "\n",
    "# Get the document for which the terms appears\n",
    "# term: exist in documents\n",
    "term_to_documents = defaultdict(list)\n",
    "new_to_old_idx = dict()\n",
    "\n",
    "terms = np.unique(X[:, thresholded_words_idx].nonzero()[1])\n",
    "\n",
    "# new_idx: original_feature_idx\n",
    "# original_feature_idx : [list of documents with these features]\n",
    "for i in terms:\n",
    "    new_to_old_idx[i] = thresholded_words_idx[i]\n",
    "\n",
    "# look for non zeroes\n",
    "# The first array is the row (documents)\n",
    "# The second array are the columns (terms)\n",
    "for doc, term in np.nditer(X[:, thresholded_words_idx].nonzero()):\n",
    "    original_feature_idx = new_to_old_idx.get(int(term))\n",
    "    word = feature_idx_to_word.get(original_feature_idx)\n",
    "    term_to_documents[word].append(int(doc))\n",
    "\n",
    "# TBC plots these words to the documents.\n",
    "# figure out how to convert this into a numpy array or am i just going in a circle?\n",
    "# TODO print term_to_documents get rid of this once i have written about it...\n",
    "# build a histogram of the number of documents for each term?\n",
    "\n",
    "# thresholded_words.sort_values(['tfidf'], ascending=False) # care this is the average highest tfscore for each top documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most significant term\n",
    "\n",
    "The most significant term in each row words are not that good.\n",
    "The mean tfidf score are better for a term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 5-12-17 to 6-12-17\n",
    "\n",
    "[x] keep the words above threshold\n",
    "\n",
    "[x] document the words appears in and does not appear in. -> extract the columns where there is a match to the string of words.\n",
    "\n",
    "[x] input vector must be same length and each element of the vector represents the presence of a word in the tweet.\n",
    "\n",
    "[x] convert all potential words into a vector representation e.g. One Hot Encoding\n",
    "\n",
    "[] understand what is the deal data to go into KModes.\n",
    "\n",
    "[] go through the algorithm KModes manually.\n",
    "\n",
    "[x] what does each cluster mean? i need to label the graph better with more information. -> true label plot\n",
    "\n",
    "[-] consider tf-idf score when giving the value a weight.? [speak with Ryan]\n",
    "\n",
    "issue:\n",
    "There are words that appear in more than one tag. But in order to keep this problem unsupervised I would not be able to distinguish the different. after mapping it to a value.\n",
    "\n",
    "### Explaination of why KModes performs poorly\n",
    "Why is our current data performing poorly and clustered about 1 type.\n",
    "It is because of the distance criteria. Kmodes employs a simple matching criteria e.g. the more features that have the same category value the more similiar it is. \n",
    "I need to feature engineer from the twitter messages in such a way that it points towards it subject.\n",
    "[My insight was found here](https://shapeofdata.wordpress.com/2014/03/04/k-modes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7-12-17\n",
    "* I have the document with its most significant word (highest tf-score).\n",
    "* I want to plot these words in the new graph\n",
    "* Making the dots really transparent to see these words. alpha=0.7\n",
    "\n",
    "[] make another plot for using the word in the document instead of a dot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "[]- need to evaluate accuracy of clustering algorithm. With cluster purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "\n",
    "[x] Before plotting my bag of words I need to apply a dimensionilty reduction.\n",
    "\n",
    "[x] plotted points with kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimentionality reduction for Bag of Words\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "X_threshold_TSNE = TSNE(n_components=2).fit_transform(X_threshold_binary.toarray())\n",
    "X_threshold_PCA = PCA(n_components=2).fit_transform(X_threshold_binary.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_words import plot_words_TSNE\n",
    "plot_words_TSNE(X_threshold_TSNE, X_threshold, data, term_to_documents)\n",
    "X_threshold_TSNE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "\n",
    "\\subsection{Observations}\n",
    "A possible issue with the poor clustering may be because of the short documents. Having short documents means our vector is rather short. So the folllowing implementations will be attempted to improve the clustering. Firstly, collect a lot more data, currently 500 messages has been collected across the topics included. The aim is to have 100,000 twitter messages. Secondly, use a library that is able to measure the distance between words, which will cluster words together better. The library will be adapted to find similar tweets. The overall goal for these implementations is to plot similar tweets together. An easy but necessary implementation of cluster purity performance measure will be required, because evaluating clustering by eye is difficult and is not suitable for larger number of tweets. Furthermore it is necessary to produce quantitative results for more consistent evaluations.\n",
    "\\newline\n",
    "\\newline\n",
    "clusters form the documents seem to overlap each other. not good.\n",
    "Based on the the **true label plot** I personally would say the feautures are not separated enough from each other.\n",
    "\n",
    "[] - dimensionaility reduction has not performed well on a bag of words. i should try word2vec like in the paper. \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "print X_threshold_TSNE.shape\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax0 = fig.add_subplot(321)\n",
    "ax0.scatter(X_threshold_PCA[:,0], X_threshold_PCA[:,1])\n",
    "ax0.set_title('PCA plot without labels')\n",
    "\n",
    "ax1 = fig.add_subplot(322)\n",
    "color = ('black', 'green', 'red', 'purple', 'yellow')\n",
    "for i,t in enumerate (np.unique(data['label'])):\n",
    "    idx = data[data['label'] == t].index\n",
    "    ax1.scatter(X_threshold_PCA[idx, 0], X_threshold_PCA[idx, 1], color=color[i], alpha=0.5)\n",
    "    \n",
    "ax1.set_title('PCA plot with true labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE looks more promising for plot visualisation for bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] - Word2Vec then t-SNE approach motivated by [paper](https://arxiv.org/pdf/1607.00534.pdf)\n",
    "\n",
    "[extra reading minhash](http://www.bmva.org/bmvc/2008/papers/119.pdf)\n",
    "\n",
    "[videos on LSH](https://www.youtube.com/watch?v=bQAYY8INBxg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TF-IDF as features and use t-SNE to reduce to 2 components.\n",
    "\n",
    "X_threshold_TSNE = TSNE(n_components=2).fit_transform(X_threshold.toarray())\n",
    "X_threshold_PCA = PCA(n_components=2).fit_transform(X_threshold.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_words import plot_words_TSNE\n",
    "\n",
    "plot_words_TSNE(X_threshold_TSNE, X_threshold, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 13-12-17\n",
    "\n",
    "[] track the cluster labels in kmodes_iter, **issue** kmodes is converging after 1 iteration\n",
    "    \n",
    "    [] visualise the first ten steps of KModes and explain it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode words\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "le = LabelEncoder()\n",
    "\n",
    "X_2 = le.fit_transform(X_)\n",
    "\n",
    "# reshape data since we have a single feature\n",
    "X_2 = X_2.reshape(1,-1)\n",
    "enc.fit(X_2)\n",
    "\n",
    "onehotlabels = enc.transform(X_2).toarray()\n",
    "\n",
    "onehotlabels\n",
    "\n",
    "X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RT @ #happyfuncoding: this is a typical Twitter tweet :-)\n",
      "[u'rt', u'@', u'this', u'is', u'twitter', u'a', u':', u'tweet', u'typical']\n",
      "======================================================================\n",
      "HTML entities &amp; other Web oddities can be an &aacute;cute <em class='grumpy'>pain</em> >:(\n",
      "[u'and', u'web', u'pain', u'html', u'be', u'an', u'entities', u'\\xe1cute', u'can', u'oddities', u'other']\n",
      "======================================================================\n",
      "It's perhaps noteworthy that phone numbers like +1 (800) 123-4567, (800) 123-4567, and 123-4567 are treated as words  despite their whitespace.\n",
      "[u'and', u'like', u'that', u'perhaps', u\"it's\", u'noteworthy', u',', u'.', u'phone', u'as', u'numbers', u'words', u'despite', u'treated', u'their', u'are', u'whitespace']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import tokenize\n",
    "\n",
    "\n",
    "samples = (\n",
    "        u\"RT @ #happyfuncoding: this is a typical Twitter tweet :-)\",\n",
    "        u\"HTML entities &amp; other Web oddities can be an &aacute;cute <em class='grumpy'>pain</em> >:(\",\n",
    "        u\"It's perhaps noteworthy that phone numbers like +1 (800) 123-4567, (800) 123-4567, and 123-4567 are treated as words  despite their whitespace.\")\n",
    "        \n",
    "for s in samples:\n",
    "    print \"======================================================================\"\n",
    "    print s\n",
    "#     print tok.filter(s)\n",
    "    print tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [scikit-learn TF-IDF arguments description](https://appliedmachinelearning.wordpress.com/2017/02/12/sentiment-analysis-using-tf-idf-weighting-pythonscikit-learn/)\n",
    "\n",
    "* min_df – remove the words from the vocabulary which have occurred in less than ‘min_df’ number of files.\n",
    "* max_df – remove the words from the vocabulary which have occurred in more than ‘max_df’ * total number of files in corpus.\n",
    "* sublinear_tf – scale the term frequency in logarithmic scale.(talked about this earlier).\n",
    "* stop_words – remove the predefined stop words of that language if present.\n",
    "* use_idf – weight factor must use inverse document frequency(obviously).\n",
    "* token_pattern – It is a regular expression for the kind of words chosen in vocabulary. default: u'(?u)\\b\\w\\w+\\b’ which means words only with 2 or more alphanumeric characters. If you want to keep only words with 2 or more alphabets(no numeric) then set token_pattern as ur'(?u)\\b[^\\W\\d][^\\W\\d]+\\b’  \n",
    "* max_features – choose maximum number of words to be kept in vocabulary ordered by term frequency.\n",
    "* vocabulary – If you have created your own vocabulary, give it as a list here otherwise it will generate vocabulary from the training data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
