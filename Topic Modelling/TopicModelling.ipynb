{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['129.150.114.173:27017'], document_class=dict, tz_aware=False, connect=True), 'TwitterStream'), 'topic')\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('129.150.114.173', 27017)\n",
    "db = client.TwitterStream;\n",
    "\n",
    "Topics = db[\"topic\"];\n",
    "print(Topics);\n",
    "\n",
    "\n",
    "#Get all the Tweets from Database-----------------\n",
    "Tweets = [];\n",
    "for obj in Topics.find({}):\n",
    "    Tweets.append(obj);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objects2 = Tweets[:];\n",
    "\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#Clean the Tweets or Preprocessing--------------------------\n",
    "\n",
    "tweets_cleaned = [] \n",
    "#tweet_content = db.tweet.find()\n",
    "\n",
    "#print(u\"\\U0001F600-\\U0001F64F\")\n",
    "emoji_pattern = re.compile(\n",
    "    u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "    u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "    u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "    u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "    u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "    \"+\", flags=re.UNICODE)#emoji_pattern = re.compile('\\S')\n",
    "hashtag_pattern = re.compile('\\#')#('\\#\\S+[\\s|$]')\n",
    "mention_pattern = re.compile('\\@\\S+\\s|$')\n",
    "#adding the keywords to remove the obivous noice\n",
    "bad_words = []\n",
    "\n",
    "for tweet in Tweets:\n",
    "\n",
    "    tweet['text'] = tweet['text'].lower() #change all the character into lowercase\n",
    "    tweet['text'] = re.sub(r'http\\S+','',tweet['text'])#remove url\n",
    "    tweet['text'] = mention_pattern.sub(r'',tweet['text'])#remove @username\n",
    "    #tweet['text'] = re.sub(r'&amp','',tweet['text'])#remove retweet\n",
    "    #tweet['text'] = hashtag_pattern.sub(r'',tweet['text'])#remove hashtag\n",
    "    tweet['text'] = emoji_pattern.sub(r'',tweet['text'])#remove emoji\n",
    "    if not any(bad_word in tweet['text'] for bad_word in bad_words):\n",
    "        #print(tweet['text'])#remove obivous noice\n",
    "        tweets_cleaned.append(tweet);\n",
    "        #if you want to save back into mongodb, define another collection first  \n",
    "        #print(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.099*\"trump\" + 0.064*\"rt\"'), (1, '0.043*\"rt\" + 0.019*\"sick\"'), (2, '0.031*\"rt\" + 0.027*\"football\"'), (3, '0.048*\"rt\" + 0.015*\"book\"'), (4, '0.067*\"rt\" + 0.061*\"food\"'), (5, '0.048*\"rt\" + 0.014*\"trump\"'), (6, '0.048*\"rt\" + 0.038*\"trump\"'), (7, '0.060*\"rt\" + 0.026*\"retweet\"'), (8, '0.067*\"trump\" + 0.062*\"rt\"'), (9, '0.072*\"trump\" + 0.054*\"rt\"')]\n"
     ]
    }
   ],
   "source": [
    "#------------------ Apply the LDA Model --------------------\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc['text']).split() for doc in tweets_cleaned]\n",
    "\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "### Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------- Get all the topics ------------------------\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=2);\n",
    "print(topics);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.  3.  3.  9.  9.  4.  9.  4.  9.  9.  9.  6.  9.  9.  9.  6.  9.  3.\n",
      "  2.  5.  9.  9.  4.  9.  9.  1.  4.  4.  0.  4.  9.  6.  9.  9.  9.  4.\n",
      "  9.  5.  9.  6.  3.  3.  3.  9.  9.  4.  4.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "#------- Associate each Tweet a Topic by its maximmum probability ------\n",
    "\n",
    "import numpy as np\n",
    "#get topics for each document\n",
    "topics_documents = np.zeros(len(tweets_cleaned));\n",
    "\n",
    "for i in range(len(objects2)):\n",
    "    bow = dictionary.doc2bow(tweets_cleaned[i]['text'].split())\n",
    "    #get the maximmum topic probability of each document\n",
    "    distribution_topic = ldamodel.get_document_topics(bow);\n",
    "    topics_documents[i] = distribution_topic.index(max(distribution_topic));\n",
    "\n",
    "print(topics_documents[1:50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created      TopicModelled\n",
       "April 10 19  0.0                29\n",
       "             1.0                57\n",
       "             2.0               364\n",
       "             3.0               562\n",
       "             4.0               698\n",
       "             5.0               470\n",
       "             6.0               293\n",
       "             7.0               106\n",
       "             8.0                19\n",
       "             9.0              1969\n",
       "April 10 20  0.0                26\n",
       "             1.0                82\n",
       "             2.0               571\n",
       "             3.0               998\n",
       "             4.0              1408\n",
       "             5.0              1012\n",
       "             6.0               694\n",
       "             7.0               273\n",
       "             8.0                76\n",
       "             9.0              4435\n",
       "April 11 19  1.0                 3\n",
       "             2.0                61\n",
       "             3.0               171\n",
       "             4.0               375\n",
       "             5.0               417\n",
       "             6.0               423\n",
       "             7.0               107\n",
       "             8.0                18\n",
       "             9.0              1031\n",
       "April 11 20  1.0                 3\n",
       "                              ... \n",
       "April 12 20  7.0               181\n",
       "             8.0                70\n",
       "             9.0              1744\n",
       "April 13 19  1.0                10\n",
       "             2.0               116\n",
       "             3.0               448\n",
       "             4.0               627\n",
       "             5.0               767\n",
       "             6.0               524\n",
       "             7.0               147\n",
       "             8.0                70\n",
       "             9.0              1438\n",
       "April 13 20  0.0                 1\n",
       "             1.0                35\n",
       "             2.0               141\n",
       "             3.0               582\n",
       "             4.0              1330\n",
       "             5.0              1343\n",
       "             6.0              1095\n",
       "             7.0               301\n",
       "             8.0               106\n",
       "             9.0              2708\n",
       "April 13 21  2.0                 2\n",
       "             3.0                 9\n",
       "             4.0                23\n",
       "             5.0                27\n",
       "             6.0                26\n",
       "             7.0                 8\n",
       "             8.0                 6\n",
       "             9.0                69\n",
       "Name: TopicModelled, Length: 84, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------- Final Data Wanted ----------\n",
    "#Dimensions: Topic - Number of Tweets - Day/Hour\n",
    "\n",
    "#data.groupby(['month', 'item'])['date'].count()\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import dateutil\n",
    "\n",
    "data_frame = pd.DataFrame(tweets_cleaned);\n",
    "data_frame['TopicModelled'] = topics_documents;\n",
    "#data_frame['created'] = data_frame['created'].apply(datetime.datetime.strptime, args=('%A %B %d %H:%M:%S +0000 %Y',))\n",
    "data_frame['created'] = data_frame['created'].apply(dateutil.parser.parse)\n",
    "#data_frame['created'] = data_frame['created'].apply(date.strftime(\"%d/%m/%y\"))\n",
    "\n",
    "#Choose the right grouping of date by\n",
    "#%B - Month; %d - day; %H - Hour \n",
    "format_date_group = '%B %d %H';\n",
    "aggregated_data = data_frame.groupby([data_frame['created'].dt.strftime(format_date_group), 'TopicModelled'])['TopicModelled'].count();\n",
    "\n",
    "#----------- Data Frame with all the data needed --------------\n",
    "aggregated_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
